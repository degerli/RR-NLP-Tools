{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import\n",
    "\n",
    "We utilize the gensim library for topic modeling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing\n",
    "import operator\n",
    "from operator import methodcaller\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import string\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import HdpModel\n",
    "\n",
    "# plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sci-kit\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "Specify pathway to R&R program output. Select terms to blacklist and levels of terms to consider. The tokens will be loaded into a corpus and a dictionary will be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [t.strip() for t in next(csv.reader(open(\"tools\\\\blacklist.csv\", 'r')))]\n",
    "levels = [1, 2, 3]\n",
    "\n",
    "inPath = \"raw.csv\"\n",
    "\n",
    "inFile = open(inPath, 'r')\n",
    "inReader = csv.reader(inFile)\n",
    "\n",
    "docTokens = dict()\n",
    "\n",
    "\n",
    "next(inReader)\n",
    "for inRow in inReader:\n",
    "    term = inRow[0]\n",
    "    sentence = inRow[2]\n",
    "    docID = inRow[3]\n",
    "    \n",
    "    token = \"_\".join([t for t in term.split(\":\") if re.match(r'[^\\W\\d]*$', t) and not t in blacklist])\n",
    "    \n",
    "    level = token.count(\"_\")\n",
    "    \n",
    "    if level in levels and not token in blacklist and len(token) > 0:\n",
    "        if docID in docTokens:\n",
    "            docTokens[docID] += [token]\n",
    "        else:\n",
    "            docTokens[docID] = [token]\n",
    "\n",
    "docIDs = list(docTokens.keys())\n",
    "data = list(docTokens.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=0.001, max_features=200000, use_idf=True)\n",
    "\n",
    "weights = vectorizer.fit_transform([\" \".join(tokens) for tokens in data])\n",
    "\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "Get top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(rownum, weights, features, top_k=1):\n",
    "    weight_vec = weights.toarray()[rownum,:]\n",
    "    top_idx = np.argsort(weight_vec)[::-1][:top_k]\n",
    "    return [features[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=5, algorithm='auto')\n",
    "\n",
    "nnfitted = nn.fit(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "\n",
    "BASE_URL = 'http://dx.doi.org/'\n",
    "\n",
    "def getTitle(doi):\n",
    "    url = BASE_URL + doi\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header('Accept', 'application/x-bibtex')\n",
    "    try:\n",
    "        with urllib.request.urlopen(req) as f:\n",
    "            bibtex = f.read().decode()\n",
    "        start = bibtex.find(\"title = {\")\n",
    "        end = bibtex.find(\"},\", start)\n",
    "        return bibtex[start + 9:end]\n",
    "        \n",
    "        \n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            return('DOI not found.')\n",
    "        else:\n",
    "            return('Service unavailable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_papers(row, kNNmodel, tfidf_weights, tfidf_features, data):\n",
    "    keywords = get_top_features(row, tfidf_weights, tfidf_features)\n",
    "    dist,idx = kNNmodel.kneighbors(tfidf_weights[row,:])\n",
    "    idx = list(idx[0])\n",
    "    return (idx, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = input(\"DocID to query?: \")\n",
    "queryTitle = getTitle(query)\n",
    "\n",
    "row = docIDs.index(query)\n",
    "print(row)\n",
    "indices, keywords = find_nearest_papers(row, nnfitted, weights, features, data)\n",
    "indices.remove(row)\n",
    "titles = [getTitle(docIDs[index]) for index in indices]\n",
    "\n",
    "print(\" \")\n",
    "print(\"For your document: \" + queryTitle)\n",
    "\n",
    "print(\" \")\n",
    "print(\"We found the following documents: \")\n",
    "\n",
    "print(titles)\n",
    "\n",
    "print(\" \")\n",
    "print(\"And the following keywords: \")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
