{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searches\n",
    "---\n",
    "\n",
    "This programs uses R&R terms to conduct TF-IDF Nearest Neighbor searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# sci-kit\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# URLs\n",
    "import sys\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# interfacing\n",
    "from ipywidgets import widgets, Output\n",
    "from IPython.display import display, Latex, Math, clear_output, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "Specify pathway to R&R program output. Select terms to blacklist and levels of terms to consider. All tokens are loaded into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [t.strip() for t in next(csv.reader(open(\"tools\\\\blacklist.csv\", 'r')))]\n",
    "levels = [0, 1, 2, 3]\n",
    "\n",
    "inPath = \"raw.csv\"\n",
    "\n",
    "\n",
    "\n",
    "inFile = open(inPath, 'r')\n",
    "inReader = csv.reader(inFile)\n",
    "\n",
    "docTokens = dict()\n",
    "\n",
    "next(inReader)\n",
    "for inRow in inReader:\n",
    "    term = inRow[0]\n",
    "    sentence = inRow[2]\n",
    "    docID = inRow[3]\n",
    "    \n",
    "    token = \"_\".join([t for t in term.split(\":\") if re.match(r'[^\\W\\d]*$', t) and not t in blacklist])\n",
    "    \n",
    "    level = token.count(\"_\")\n",
    "    \n",
    "    if level in levels and not token in blacklist and len(token) > 0:\n",
    "        if docID in docTokens:\n",
    "            docTokens[docID] += \" \" + token\n",
    "        else:\n",
    "            docTokens[docID] = token\n",
    "\n",
    "docIDs = list(docTokens.keys())\n",
    "data = list(docTokens.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "Transforms the documents into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=0, max_features=200000, use_idf=True)\n",
    "\n",
    "weights = vectorizer.fit_transform(data)\n",
    "\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "\n",
    "We fit our data to a nearest neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=6, algorithm='auto')\n",
    "\n",
    "nnfitted = nn.fit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searches\n",
    "\n",
    "Some helper methods and a search interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://dx.doi.org/'\n",
    "\n",
    "def getTitle(doi):\n",
    "    url = BASE_URL + doi\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header('Accept', 'application/x-bibtex')\n",
    "    try:\n",
    "        with urllib.request.urlopen(req) as f:\n",
    "            bibtex = f.read().decode()\n",
    "        start = bibtex.find(\"title = {\")\n",
    "        end = bibtex.find(\"},\", start)\n",
    "        return bibtex[start + 9:end]\n",
    "        \n",
    "        \n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            return('DOI not found.')\n",
    "        else:\n",
    "            return('Service unavailable.')\n",
    "        \n",
    "def get_top_features(rownum, weights, features, top_k=10):\n",
    "    weight_vec = weights.toarray()[rownum,:]\n",
    "    top_idx = np.argsort(weight_vec)[::-1][:top_k]\n",
    "    return [features[i] for i in top_idx]\n",
    "\n",
    "def find_nearest_papers(row, kNNmodel, tfidf_weights, tfidf_features, data):\n",
    "    keywords = get_top_features(row, tfidf_weights, tfidf_features)\n",
    "    dist,idx = kNNmodel.kneighbors(tfidf_weights[row,:])\n",
    "    idx = list(idx[0])\n",
    "    return (idx, keywords)\n",
    "\n",
    "def search(query):\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    queryTitle = getTitle(query)\n",
    "\n",
    "    row = docIDs.index(query)\n",
    "    \n",
    "    indices, keywords = find_nearest_papers(row, nnfitted, weights, features, data)\n",
    "    \n",
    "    if row in indices:\n",
    "        indices.remove(row)\n",
    "    titles = [getTitle(docIDs[index]) for index in indices]\n",
    "    \n",
    "    print(\"For your document: \" + queryTitle)\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"We found the following documents: \")\n",
    "\n",
    "    for title in titles:\n",
    "        print(\"- \" + title)\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"And the following keywords: \")\n",
    "    print(str(keywords).strip(\"[]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0cf3d98c1a79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mDOI\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocIDs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-efe1994bbff1>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocIDs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_nearest_papers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnnfitted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-efe1994bbff1>\u001b[0m in \u001b[0;36mfind_nearest_papers\u001b[1;34m(row, kNNmodel, tfidf_weights, tfidf_features, data)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_nearest_papers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkNNmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_top_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkNNmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-efe1994bbff1>\u001b[0m in \u001b[0;36mget_top_features\u001b[1;34m(rownum, weights, features, top_k)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_top_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrownum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mweight_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrownum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mtop_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m         \u001b[0m_sparsetools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_todense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "for DOI in docIDs:\n",
    "    search(DOI)\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
